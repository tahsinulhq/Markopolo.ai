{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Defining paths\n",
    "output_dir = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Markopolo.ai\\\\LLM\\\\results\"\n",
    "data_file = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Markopolo.ai\\\\LLM\\\\watch_data.jsonl\"\n",
    "\n",
    "# Ensuring the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    try:\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Directory {output_dir} created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {output_dir}: {e}\")\n",
    "\n",
    "# Loading the dataset and splitting it into 80% training and 20% testing\n",
    "try:\n",
    "    train_dataset = load_dataset('json', data_files=data_file, split='train[:80%]')\n",
    "    test_dataset = load_dataset('json', data_files=data_file, split='train[80%:]')\n",
    "    print(\"Dataset loaded and split successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and split successfully.\n",
      "Tokenization completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82243603bd0a4d83b3979f91140e0b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ca77a15a9b427aa84e5d5c879370b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.837031364440918, 'eval_runtime': 35.5068, 'eval_samples_per_second': 1.183, 'eval_steps_per_second': 0.591, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253e8db12c98452e9b1cdddf2ccd892c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.083439350128174, 'eval_runtime': 51.4107, 'eval_samples_per_second': 0.817, 'eval_steps_per_second': 0.408, 'epoch': 2.0}\n",
      "{'train_runtime': 1230.8234, 'train_samples_per_second': 0.273, 'train_steps_per_second': 0.136, 'train_loss': 4.656118483770461, 'epoch': 2.0}\n",
      "Model training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading a pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Setting the pad_token to a new special token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))  \n",
    "\n",
    "# Defining the tokenization function with labels\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [\n",
    "        \"User: \" + prompt + \"\\nModel: \" + response\n",
    "        for prompt, response in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    # Tokenizing with padding and return attention mask\n",
    "    tokenized_data = tokenizer(combined_texts, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    \n",
    "    # Setting the labels to be the same as input_ids\n",
    "    tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"].clone()\n",
    "    # Replacing padding tokens in labels with -100\n",
    "    tokenized_data[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
    "        for labels in tokenized_data[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Tokenizing the datasets\n",
    "try:\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization completed successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError during tokenization: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Setting up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=output_dir,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Initializing the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tuning the model\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Model training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: what's the price of Fossil watch?\n",
      "Fossils Watch: $ The cost and specifications for this item are as follows. Color, Material (Model): Stainless Steel or Black Leather Band with Automatic Dial - N/A Retail Price : USD$39 Quantity(s) /w Display Details... Item has an approximate retail value where indicated.. DO NOT print This information is currently accurate . Information on condition Does Not Contain Mechanical Reproduction Control -- Machine wash only., Simple to clean Instructions ...~ Mint Condition Rating indicates a model that was last updated on by its manufacturer in accordance Men & Womens Chronograph Watches Series 2000 through\n"
     ]
    }
   ],
   "source": [
    "# Function to interact with the fine-tuned model\n",
    "def chat_with_model():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate output with attention mask\n",
    "        outputs = model.generate(\n",
    "            inputs, \n",
    "            max_length=150, \n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            do_sample=True,\n",
    "            temperature=0.7,  # temperature for randomness\n",
    "            top_k=50,  # Limit to the top-k most likely tokens\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Model: {response}\")\n",
    "\n",
    "# Start the chat\n",
    "chat_with_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
