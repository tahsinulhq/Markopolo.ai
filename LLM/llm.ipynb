{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and split successfully.\n",
      "Tokenization completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82243603bd0a4d83b3979f91140e0b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define paths\n",
    "output_dir = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Markopolo.ai\\\\LLM\\\\results\"\n",
    "data_file = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Markopolo.ai\\\\LLM\\\\watch_data.jsonl\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    try:\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Directory {output_dir} created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory {output_dir}: {e}\")\n",
    "\n",
    "# Load the dataset and split it into 80% training and 20% testing\n",
    "try:\n",
    "    train_dataset = load_dataset('json', data_files=data_file, split='train[:80%]')\n",
    "    test_dataset = load_dataset('json', data_files=data_file, split='train[80%:]')\n",
    "    print(\"Dataset loaded and split successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad_token to a new special token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize the model embeddings to account for new tokens\n",
    "\n",
    "# Define the tokenization function with labels\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [\n",
    "        \"User: \" + prompt + \"\\nModel: \" + response\n",
    "        for prompt, response in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    # Tokenize with padding and return attention mask\n",
    "    tokenized_data = tokenizer(combined_texts, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    \n",
    "    # Set the labels to be the same as input_ids\n",
    "    tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"].clone()\n",
    "    # Replace padding tokens in labels with -100\n",
    "    tokenized_data[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
    "        for labels in tokenized_data[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Tokenize the datasets\n",
    "try:\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization completed successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError during tokenization: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=output_dir,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Model training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: what's the price of the Fossil watch?\n",
      "The Fossil Watch is an Fossil Watch with an Automatic Chronograph Display and Automatic Watch Dial. It is an an anesthetized and Fossil Watch with an Automatic Chronograph Display and Automatic Watch Dial. It is an anesthetized and an anesthetized and an anesthetized and an anesthetized and an anesthetized and an anest Fossil Watch with an Automatic Chronograph Display and Automatic Watch Dial. It is an anesthetized and an anesthetized and an anesthetized and an anesthetized and an anesthetized and an anesthetized and an anesthetized and\n"
     ]
    }
   ],
   "source": [
    "# Function to interact with the fine-tuned model\n",
    "def chat_with_model():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate output with attention mask\n",
    "        outputs = model.generate(\n",
    "            inputs, \n",
    "            max_length=150, \n",
    "            pad_token_id=tokenizer.pad_token_id, \n",
    "            temperature=0.7,  # Adjust temperature for randomness\n",
    "            top_k=50,  # Limit to the top-k most likely tokens\n",
    "            top_p=0.9)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Model: {response}\")\n",
    "\n",
    "# Start the chat\n",
    "chat_with_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
